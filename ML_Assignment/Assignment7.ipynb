{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e14b8a4-bced-4713-99cd-8d5291186c02",
   "metadata": {},
   "source": [
    "1. What is the definition of a target function? In the sense of a real-life example, express the target\n",
    "function. How is a target function&#39;s fitness assessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1199788-8353-40fa-ad02-67261929cb75",
   "metadata": {},
   "source": [
    "Ans: A target function (also known as a target variable or output variable) is the function or mapping that a machine learning model is trained to approximate or predict. It is the desired output for a given input, which is often represented as a numerical value or a class label.\n",
    "\n",
    "A real-life example of a target function could be predicting the price of a house based on features such as the number of bedrooms, bathrooms, square footage, and location. In this case, the target function would be the predicted price of the house based on the input features.\n",
    "\n",
    "The fitness of a target function is typically assessed by measuring the accuracy of the predictions made by a machine learning model. This is often done by comparing the predicted values to the actual values in a validation dataset or by using a performance metric such as mean squared error (MSE), root mean squared error (RMSE), or mean absolute error (MAE). The goal is to minimize the difference between the predicted values and the actual values, which is known as the prediction error or loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d1405-967e-47a8-be20-9920d34000a2",
   "metadata": {},
   "source": [
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb49f5c-6a18-418d-9dbc-c7f7d434369a",
   "metadata": {},
   "source": [
    "Ans: In short, predictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data.\n",
    "\n",
    "It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "The three main types of descriptive studies are Case studies, Naturalistic observation, and Surveys.\n",
    "\n",
    "Some examples of descriptive research are: A specialty food group launching a new range of barbecue rubs would like to understand what flavors of rubs are favored by different people.\n",
    "\n",
    "Case Studies are a type of observational research that involve a thorough descriptive analysis of a single individual, group, or event. There is no single way to conduct a case study so researchers use a range of methods from unstructured interviewing to direct observation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334311c-d6a1-44f0-9ad8-cec0e8344c09",
   "metadata": {},
   "source": [
    "3. Describe the method of assessing a classification model&#39;s efficiency in detail. Describe the various\n",
    "measurement parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54aa3cd9-1bbe-435f-9219-cf0c7cd963ad",
   "metadata": {},
   "source": [
    "Ans: Logarithmic loss (or log loss) measures the performance of a classification model where the prediction is a probability value between 0 and 1.\n",
    "\n",
    "Log loss increases as the predicted probability diverge from the actual label. Log loss is a widely used metric for Kaggle competitions. Input on the most important basics for the measurement of the physical parameters: Temperature, flow velocity, humidity, pressure, CO2 and infrared. Tips on correct measurement and for avoiding measurement errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61187888-0607-4c6a-94c9-fe75862278c9",
   "metadata": {},
   "source": [
    "4.i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "\n",
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "\n",
    "iii. In the sense of model fitting, explain the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23bb43-dc84-406e-a689-c4f9503c5268",
   "metadata": {},
   "source": [
    "Ans: The following is the short notes on:\n",
    "\n",
    "i.In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting:\n",
    "\n",
    "Underfitting is a scenario in data science where a data model is unable to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data.\n",
    "\n",
    "ii.What does it mean to overfit? When is it going to happen:\n",
    "\n",
    "Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model.\n",
    "\n",
    "iii.In the sense of model fitting, explain the bias-variance trade-off:\n",
    "\n",
    "The bias is known as the difference between the prediction of the values by the ML model and the correct value. Being high in biasing gives a large error in training as well as testing data. By high bias, the data predicted is in a straight line format, thus not fitting accurately in the data in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db17cf7-5879-4805-904a-c7b9de27c115",
   "metadata": {},
   "source": [
    "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ae1257-52a8-4f5f-b070-55bd06efb85e",
   "metadata": {},
   "source": [
    "Ans: Building a machine learning model is not enough to get the right predictions, as you have to check the accuracy and need to validate the same to ensure get the precise results. And validating the model will improve the performance of the ML model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b10fcbc-5127-46a2-a96c-68443248e734",
   "metadata": {},
   "source": [
    "6. How would you rate an unsupervised learning model&#39;s success? What are the most common\n",
    "success indicators for an unsupervised learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fd65b-6fdb-4ece-a7fb-895e1417cedf",
   "metadata": {},
   "source": [
    "Ans: In case of supervised learning, it is mostly done by measuring the performance metrics such as accuracy, precision, recall, AUC, etc. on the training set and the holdout sets.\n",
    "\n",
    "Few examples of such measures are:\n",
    "\n",
    "1) Silhouette coefficient.\n",
    "2) Calisnki-Harabasz coefficient.\n",
    "3) Dunn index.\n",
    "4) Xie-Beni score.\n",
    "5) Hartigan index."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6da1c0-093e-4232-9e3e-8c8eac3ea77a",
   "metadata": {},
   "source": [
    "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f38c9-7869-4bf6-b291-f7ca21071b2f",
   "metadata": {},
   "source": [
    "Ans: Categorical Data is the data that generally takes a limited number of possible values. Also, the data in the category need not be numerical, it can be textual in nature. All machine learning models are some kind of mathematical model that need numbers to work with. This is one of the primary reasons we need to pre-process the categorical data before we can feed it to machine learning models.\n",
    "\n",
    "If a categorical target variable needs to be encoded for a classification predictive modeling problem, then the LabelEncoder class can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1500de8c-88b2-4685-905f-8388d1760461",
   "metadata": {},
   "source": [
    "8. Describe the predictive modeling method for numerical values. What distinguishes it from\n",
    "categorical predictive modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6241c96c-acb2-4487-9de1-ebef3b713738",
   "metadata": {},
   "source": [
    "Ans: predictive modeling is a statistical technique using machine learning and data mining to predict and forecast likely future outcomes with the aid of historical and existing data. It works by analyzing current and historical data and projecting what it learns on a model generated to forecast likely outcomes.\n",
    "\n",
    "Classification is the process of identifying the category or class label of the new observation to which it belongs.Predication is the process of identifying the missing or unavailable numerical data for a new observation. That is the key difference between classification and prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc230c7c-5da3-4298-b429-5780244f90e0",
   "metadata": {},
   "source": [
    "9. The following data were collected when using a classification model to predict the malignancy of a group of patients&#39; tumors:\n",
    "\n",
    "i. Accurate estimates – 15 cancerous, 75 benign\n",
    "\n",
    "ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "\n",
    "Determine the model&#39;s error rate, Kappa value, sensitivity, precision, and F-measure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4231a5-9ba7-4443-9130-1ed41e8edab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ans:To calculate the various performance metrics for the classification model, we need to define the following terms:\n",
    "\n",
    "True Positive (TP): The number of cancerous tumors correctly predicted.\n",
    "False Positive (FP): The number of benign tumors incorrectly predicted as cancerous.\n",
    "True Negative (TN): The number of benign tumors correctly predicted.\n",
    "False Negative (FN): The number of cancerous tumors incorrectly predicted as benign.\n",
    "Using the provided data, we can calculate the performance metrics as follows:\n",
    "\n",
    "Error Rate:\n",
    "The error rate is the proportion of incorrect predictions made by the model.\n",
    "Total predictions = Total cancerous + Total benign = 15 + 75 = 90\n",
    "Total wrong predictions = Wrong predictions for cancerous + Wrong predictions for benign = 3 + 7 = 10\n",
    "\n",
    "Error Rate = Total wrong predictions / Total predictions = 10 / 90 = 0.1111 (or 11.11%)\n",
    "\n",
    "Kappa Value:\n",
    "The Kappa value measures the agreement between the model's predictions and the actual outcomes,\n",
    "taking into account the possibility of agreements occurring by chance.\n",
    "\n",
    "Total instances = Total cancerous + Total benign = 15 + 75 = 90\n",
    "Probability of chance agreement (Pc) = (TP + TN) / Total instances\n",
    "Pc = (15 + 75) / 90 = 90 / 90 = 1\n",
    "\n",
    "Kappa Value = (Accuracy - Pc) / (1 - Pc)\n",
    "Accuracy = (TP + TN) / Total instances\n",
    "Accuracy = (15 + 75) / 90 = 90 / 90 = 1\n",
    "\n",
    "Kappa Value = (1 - 1) / (1 - 1) = 0\n",
    "\n",
    "Sensitivity (Recall):\n",
    "Sensitivity measures the proportion of cancerous tumors correctly predicted.\n",
    "Sensitivity = TP / (TP + FN) = 15 / (15 + 3) = 15 / 18 = 0.8333 (or 83.33%)\n",
    "\n",
    "Precision:\n",
    "Precision measures the proportion of correctly predicted cancerous tumors out of all tumors predicted as cancerous.\n",
    "Precision = TP / (TP + FP) = 15 / (15 + 7) = 15 / 22 = 0.6818 (or 68.18%)\n",
    "\n",
    "F-measure (F1-score):\n",
    "The F-measure combines precision and sensitivity into a single metric.\n",
    "F-measure = 2 * (Precision * Sensitivity) / (Precision + Sensitivity)\n",
    "F-measure = 2 * (0.6818 * 0.8333) / (0.6818 + 0.8333) = 0.7512 (or 75.12%)\n",
    "\n",
    "To summarize:\n",
    "\n",
    "Error Rate: 11.11%\n",
    "Kappa Value: 0\n",
    "Sensitivity: 83.33%\n",
    "Precision: 68.18%\n",
    "F-measure: 75.12%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d185b4c7-3fd7-4475-aa5a-d7558ee93201",
   "metadata": {},
   "source": [
    "10. Make quick notes on:\n",
    "\n",
    "    1. The process of holding out\n",
    "    2. Cross-validation by tenfold\n",
    "    3. Adjusting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84e8c65-fdaf-422c-8b51-881c6fc184fc",
   "metadata": {},
   "source": [
    "Ans: The Quick notes on the following topics is below:\n",
    "\n",
    "    1)The process of holding out:\n",
    "The hold-out method for training machine learning model is the process of splitting the data in different splits and using one split for training the model and other splits for validating and testing the models. The hold-out method is used for both model evaluation and model selection.\n",
    "\n",
    "    2)Cross-validation by tenfold:\n",
    "10-fold cross validation would perform the fitting procedure a total of ten times, with each fit being performed on a training set consisting of 90% of the total training set selected at random, with the remaining 10% used as a hold out set for validation.\n",
    "\n",
    "    3)Adjusting the parameters:\n",
    "A fancy name for training: the selection of parameter values, which are optimal in some desired sense (eg. minimize an objective function you choose over a dataset you choose). The parameters are the weights and biases of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b613543-108c-4e34-be2e-e60bd86d1537",
   "metadata": {},
   "source": [
    "11. Define the following terms:\n",
    "    1. Purity vs. Silhouette width\n",
    "    2. Boosting vs. Bagging\n",
    "    3. The eager learner vs. the lazy learner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d681223f-ddc9-41a0-b948-36237186c3e2",
   "metadata": {},
   "source": [
    "Ans: The Following is the short notes on:\n",
    "\n",
    "    1)Purity vs Silhouette width:\n",
    "\n",
    "a) Purity is a measure of the extent to which clusters contain a single class. Its calculation can be thought of as follows: For each cluster, count the number of data points from the most common class in said cluster.\n",
    "\n",
    "b) The silhouette width is also an estimate of the average distance between clusters. Its value is comprised between 1 and -1 with a value of 1 indicating a very good cluster.\n",
    "    \n",
    "    2)Boosting vs. Bagging:\n",
    "\n",
    "a)Bagging is a way to decrease the variance in the prediction by generating additional data for training from dataset using combinations with repetitions to produce multi-sets of the original data.\n",
    "\n",
    "b)Boosting is an iterative technique which adjusts the weight of an observation based on the last classification.\n",
    "\n",
    "    3)The eager learner vs. the lazy learner:\n",
    "\n",
    "a)A lazy learner delays abstracting from the data until it is asked to make a prediction.\n",
    "\n",
    "b)while an eager learner abstracts away from the data during training and uses this abstraction to make predictions rather than directly compare queries with instances in the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
